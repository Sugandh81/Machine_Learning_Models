Linear Regression and Stochastic Gradient Descent


In addition to gradient descent (GD), stochastic gradient descent (SGD) is another optimization algorithm which can be utilized to minimize an objective function. We will implement SGD to minimize the mean-squared error (MSE) for fitting a linear regression. We will use [NASA 'airfoil' dataset](https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise#) and compare SGD to the GD algorithm we implemented.




Objectives 
- Implement the closed-form solution to the Ordinary Least Squares problem
- Implement and apply stochastic gradient descent algorithm to linear regression
- Compare various learning rates for the SGD algorithm
- Compare the efficiency and convergence rate of GD and SGD 

Please refer img folder

Prerequisite Python 3